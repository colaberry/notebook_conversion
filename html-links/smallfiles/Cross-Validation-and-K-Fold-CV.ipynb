{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "#### What is Cross Validation ?\n",
    "Cross Validation is a practice of <strong>tuning</strong> your <strong>predictive model</strong> to prevent the models from <strong>overfitting</strong> or <strong>underfitting</strong> a dataset.  \n",
    "\n",
    "Let's dive deeper as to what the definitions mean and we will begin with the term predictive model.\n",
    "<ul>\n",
    "    <li>Predictive Model</li>\n",
    "    <small>It refers to an alogorithm which discovers new patterns in a dataset.</small>\n",
    "    <hr>\n",
    "    <li>Tuning</li>\n",
    "    <small>The concept of making your Model make better predictions. </small>\n",
    "    <hr>\n",
    "    <li>Overfitting</li>\n",
    "    <small>The idea that your algorithm/model becomes too fimiliar with only a centaian set of training data. Making the model a failure with other types of datasets.</small>\n",
    "    <hr>\n",
    "    <li>Underfit</li>\n",
    "    <small>The idea that only your model isn't good enough and needs more tuning</small>\n",
    "</ul>\n",
    "\n",
    "The practice of Cross-validation begins by spitting conditioned dataset for train-test dataset. This is a traditional practice followed in a professional environment.<br>\n",
    "We can split the dataset either:- \n",
    "<ul>\n",
    "    <li>80/20 where 80% are training dataset and rest 20% are the test dataset</li>\n",
    "    <li>70/30 where 70% are training dataset and rest 30% are the test dataset</li>\n",
    "</ul>\n",
    "\n",
    "The above 2 scenarios are mostly used in cross validation. <br>\n",
    "But we have the freedom to experiment with different type so of combinations, it advised not to take 50/50 because it leaves the model unfinished or an underfitting model.\n",
    "\n",
    "Cross Validation helps with understanding how your model might perform with new datasets. <br>\n",
    "And in the following example we shall see how to implement it.\n",
    "<ul>\n",
    "    <li>We will begin by import all the required modules</li>\n",
    "    <li>Importing the Wine dataset, the dataset contains 13 features which reflect which class of wine a certain data point is. load_wine() exists in the sklearn module.</li>\n",
    "</ul>\n",
    "\n",
    "```Python\n",
    "from sklearn.datasets import load_wine\n",
    "```\n",
    "<ul>\n",
    "    <li>Importing the classifier (pre-defined algorithm for running an analysis).</li>\n",
    "    <li>To be more specific we will be using a naive bayes classifier. The classifier was randomly selected. </li>\n",
    "</ul>\n",
    "\n",
    "```Python\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "```\n",
    "<ul>\n",
    "    <li>The following modules are the main reason for Cross Validation.</li>\n",
    "    <li>train_test_split helps with splitting the dataset into either 70/30, 80/20 etc.</li>\n",
    "    <li>confusion_matrix helps with calculating the accuracy, which we will implement further on</li>\n",
    "</ul>\n",
    "\n",
    "```Python\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "```\n",
    "\n",
    "<ul>\n",
    "    <li>Extracting the data from the module</li> \n",
    "    <li><i>data</i> variable contains the 13 features for the wine in a numpy 2d array</li>\n",
    "    <li><i>target</i> variable contains the class (0,1,2) of each set of features in the corresponding index</li>\n",
    "</ul>\n",
    "\n",
    "```Python\n",
    "data = load_wine()['data']\n",
    "target = load_wine()['target']\n",
    "```\n",
    "<ul>\n",
    "    <li>We instantiate our classifier here</li>\n",
    "    <li>This classifier allows us to analyse our dataset by using its functions </li>\n",
    "    <li>This would be considered an example of a predictive model</li>\n",
    "\n",
    "</ul>\n",
    "\n",
    "```Python\n",
    "clNB = GaussianNB()\n",
    "```\n",
    "\n",
    "<ul>\n",
    "    <li>Line 1: This is the first at training our model where we are splitting the dataset into train and test dataset</li>\n",
    "    <small>In this case we have <i>train_w</i> and <i>target_train_w</i>\n",
    "    which the naming convention suggests comprise of the data which we going to use to train our model with. And the variables <i>test_w</i> and <i>target_test_w</i> will be used to make our test data  </small>.\n",
    "    <img src='cv.PNG' width=\"150\" height=\"150\">\n",
    "    <li>Line 2: We use our classifier to fit the data and we use do it through .fit method</li>\n",
    "    <small>.fit() method is used by almost all the classifiers, the basic idea here is that the model is trying to familiarize itself with the data. Similar to it you might also encounter .fit_transform() goes on a further step to standardize it.</small> \n",
    "    <li>Line 3: Here we have 2 processes to think about. </li>\n",
    "    <small>\n",
    "        <ol>\n",
    "            <li>.predict() method inside of the confusion matrix basically returs a list of predict values.</li>\n",
    "            <li>.confusion_matrix() returns a 2D array whose sum of diagonal values reflect the correctly predicted values and estimate the accuracy.</li>\n",
    "        </ol>\n",
    "    </small>\n",
    "</ul>\n",
    "\n",
    "\n",
    "\n",
    "```Python\n",
    "# Line 1 #\n",
    "train_w, test_w, target_train_w, target_test_w = train_test_split(data, target, test_size=0.25, random_state=0)\n",
    "# Line 2 #\n",
    "clNB = clNB.fit(train_w, target_train_w)\n",
    "# Line 3 #\n",
    "matrix = confusion_matrix(clNB.predict(test_w), target_test_w)\n",
    "print(\"Bayes' Analysis\",  matrix)\n",
    "print('Accuracy {:.2f}'.format((matrix[0][0]+matrix[1][1]+matrix[2][2])/len(test_w) * 100))\n",
    "```\n",
    "The Results\n",
    "\n",
    "<img src='cm.PNG' height=\"150\" width=\"150\">\n",
    "<img src='acc.PNG'height=\"150\" width=\"150\">\n",
    "\n",
    "Our trained model managed to get an accuracy of 93%. But a better practice is for running a cross-validation. <br>\n",
    "Since, this dataset is small it could be that our model might have overfitted to the data that was provided in the test set<br>\n",
    "meaning in a hypothetical scnario with more testing samples our model wouldn't work that well.  <br>\n",
    "\n",
    "For that reason we will cross-validate our model.\n",
    "\n",
    "<ol>\n",
    "    <li>We begin by spliting the training data further into training and test sets</li>\n",
    "    <li>We will then create a new model for analysis and fit our constructed data</li>\n",
    "    <li>Then we run an analysis on the constructed dataset and check our accuracy</li>\n",
    "    <li>After which we can use this model to check the accuracy on the main dataset</li>\n",
    "</ol>\n",
    "\n",
    "<img src='KCV.PNG' height=\"300\" width=\"500\">\n",
    "\n",
    "```Python\n",
    "# 1.\n",
    "train_cv_w, test_cv_w, target_cv_train_w, target_cv_test_w = train_test_split(train_w, target_train_w, test_size=0.25, random_state=0)\n",
    "# 2.\n",
    "clNB1 = GaussianNB()\n",
    "clNB1 = clNB1.fit(train_cv_w, target_cv_train_w)\n",
    "\n",
    "# 3.\n",
    "matrix1 = confusion_matrix(clNB1.predict(test_cv_w), target_cv_test_w)\n",
    "print('Bayes CV Analysis')\n",
    "print(matrix1)\n",
    "print('Accuracy {:.2f}'.format((matrix1[0][0]+matrix1[1][1]+matrix1[2][2])/len(test_w) * 100))\n",
    "```\n",
    "The Results<br>\n",
    "<img src='CV.PNG' height=\"150\" width=\"150\">\n",
    "\n",
    "Gives a good idea that our is working.\n",
    "\n",
    "\n",
    "```Python\n",
    "# 4.\n",
    "matrix2 = confusion_matrix(clNB1.predict(test_w), target_test_w)\n",
    "print(\"Bayes' Analysis\",  matrix2)\n",
    "print('Accuracy {:.2f}'.format((matrix2[0][0]+matrix2[1][1]+matrix2[2][2])/len(test_w) * 100))\n",
    "```\n",
    "The Results<br>\n",
    "\n",
    "<img src='PostKCV.PNG' height=\"150\" width=\"150\">\n",
    "\n",
    "\n",
    "\n",
    "The results give us a confidence that our model is atleast not overfitting but in the next we should learn how to make this classifier even better.\n",
    "\n",
    "\n",
    "\n",
    "Exercise: Given an instantiated classifier run a cross verified analysis on the established dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "data = load_breast_cancer()['data']\n",
    "target = load_breast_cancer()['target']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "#### Solution\n",
    "\n",
    "```Python\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "data = load_breast_cancer()['data']\n",
    "target = load_breast_cancer()['target']\n",
    "\n",
    "cls = KNeighborsClassifier(n_neighbors=3)\n",
    "train, test, target_train, target_test = train_test_split(data, target, test_size=0.25, random_state=0)\n",
    "train_cv_w, test_cv_w, target_cv_train_w, target_cv_test_w = train_test_split(train, target_train, test_size=0.25, random_state=0)\n",
    "\n",
    "cls = cls.fit(train_cv_w, target_cv_train_w)\n",
    "result = cls.predict(test_cv_w)\n",
    "\n",
    "accuracy = confusion_matrix(target_cv_test_w, result)\n",
    "accuracy = (accuracy[0][0]+accuracy[1][1])/len(target_cv_test_w)\n",
    "accuracy = accuracy*100\n",
    "\n",
    "print(\"Cross Validated Accuracy: {:.2f}\".format(accuracy))\n",
    "\n",
    "final_result = cls.predict(test)\n",
    "accuracy = confusion_matrix(target_test, final_result)\n",
    "accuracy = (accuracy[0][0]+accuracy[1][1])/len(target_test)\n",
    "accuracy = accuracy*100\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold Cross Validation\n",
    "#### What does K-Fold mean ?\n",
    "<i>K</i> refers to the splits we can make to the training set.\n",
    "The concept here is that we can essentially train our model more than once with different datasets. <br>\n",
    "<ol>\n",
    "    <li>We can first split the training dataset into different buckets, and each bucket holds an equal number of training datapoints (the bucket length is handled by a method and the how its split will handled automatically so you don't have to worry about length of any object the remainder datapoints will be taken care of)</li>\n",
    "    <img src=\"KFold.png\" height=\"300\" width=\"500\">\n",
    "    <li>Then we loop through through the list of buckets and then we .fit and .predict each one of them to tally their accuracy</li>\n",
    "    <img src=\"KFold1.png\" height=\"300\" width=\"500\">\n",
    "    <li>At the end, we tend to use this trained model for our final prediction</li>\n",
    "</ol>\n",
    "\n",
    "Let's go back to iur example for Cross-Validation try to make a K-fold cross verified model and then compare our results. \n",
    "\n",
    "<ul>\n",
    "    <li>Import the Libraries you want</li>\n",
    "    <li>KFold: This module is used to make buckets of our training data. this works similar with train__test_split() but in this case it tries to split the data in exactly the number of splits mentioned. In this case it splits in terms of indices and we will see how we can use them </li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "```Python\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neighbors import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "```\n",
    "\n",
    "<ul>\n",
    "    <li>Loading the data</li>\n",
    "    <li>Splitting the data to make train and test set</li>\n",
    "    <li>Instantiate the classifier</li>\n",
    "</ul>\n",
    "\n",
    "```Python\n",
    "data = load_wine()['data']\n",
    "target = load_wine()['target']\n",
    "\n",
    "train_w, test_w, target_train_w, target_test_w = train_test_split(data, target, test_size=0.25, random_state=0)\n",
    "\n",
    "clNBm = GaussianNB()\n",
    "```\n",
    "\n",
    "<ul>\n",
    "    <li>We first create a KFold object. <small>Refresher: objects are entities that help with data computations assigned for that variable</small></li>\n",
    "    <li>we then use the .split() method to get a set of train_index and test_index.</li>\n",
    "    <li>On every iteration we create training and testing data and get its accuracy on that particular data making it a different dataset everytime</li>\n",
    "</ul>\n",
    "\n",
    "```Python\n",
    "kfold = KFold(n_splits=10)\n",
    "\n",
    "for train_index, test_index in kfold.split(train_w):\n",
    "    X_train, X_test = train_w[train_index], train_w[test_index]\n",
    "    y_train, y_test = target_train_w[train_index], target_train_w[test_index]\n",
    "    clNBm = clNBm.fit(X_train, y_train)\n",
    "    y_pred1 = clNBm.predict(X_test)\n",
    "    matrix1 = confusion_matrix(y_test, y_pred1)\n",
    "    print('GB',((matrix1[0][0]+matrix1[1][1]+matrix1[2][2])/len(X_test))*100)\n",
    "    \n",
    "```\n",
    "The Results<br>\n",
    "<img src=\"kfoldr.png\" height=\"150\" width=\"150\">\n",
    "\n",
    "We see 10 values for 10 splits and we can see the how these played out. \n",
    "\n",
    "Now we run our classifier on our main dataset.\n",
    "\n",
    "```Python\n",
    "pred1 = clNBm.predict(test_w)\n",
    "matrix = confusion_matrix(target_test_w, pred1)\n",
    "accuracy = (matrix[0][0]+matrix[1][1]+matrix[2][2])/len(target_test)\n",
    "accuracy = accuracy*100\n",
    "print(\"Accuracy on test set: {:.2f}\".format(accuracy))\n",
    "```\n",
    "The Results<br>\n",
    "95.56% accuracy. \n",
    "\n",
    "\n",
    "Run a KFold validation with 10 splits on a new model with the previous dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise Solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "# Solution\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "data = load_breast_cancer()['data']\n",
    "target = load_breast_cancer()['target']\n",
    "\n",
    "clg = KNeighborsClassifier(n_neighbors=3)\n",
    "train, test, target_train, target_test = train_test_split(data, target, test_size=0.25, random_state=0)\n",
    "kfold = KFold(n_splits=10)\n",
    "\n",
    "for train_index, test_index in kfold.split(train):\n",
    "    X_train, X_test = train[train_index], train[test_index]\n",
    "    y_train, y_test = target_train[train_index], target_train[test_index]\n",
    "    \n",
    "    clg = clg.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = clg.predict(X_test)\n",
    "    \n",
    "    matrix = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    print('KN',((matrix[0][0]+matrix[1][1])/len(X_test))*100)\n",
    "\n",
    "    \n",
    "matrix1 = confusion_matrix(target_test,clg.predict(test))    \n",
    "accuracy = (matrix1[0][0]+matrix1[1][1])/len(target_test)\n",
    "accuracy = accuracy*100\n",
    "print(\"Accuracy on test set: {:.2f}\".format(accuracy))\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
